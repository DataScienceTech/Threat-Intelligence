分类：

kNN

朴素贝叶斯
：选词->将实例映射为向量->计算类别概率->代码优化(对数化处理、去零等)->计算每一个元素在每一个类别中的条件概率->


决策树
:计算原始熵->遍历计算所有根据每个属性分类后的熵->对比选择最优属性->根据选定属性进行分类->对所有子类递归前面的过程
:熵的计算(p(x)*log(p(x),2)),类别概率乘以类别概率以2为底的对数,信息增益为原始熵与分类后的熵的差值,表明当前处理的数据无序度的改变

SVM

Ann(人工神经网络)

回归:

logistic回归
:sigmoid 函数的作用是归一化，外援，分类概率估计
:目标是找出一个向量 w，使得 y = wx 为一条最佳分类直线，即 sigmoid 估计概率跟已知分类值(0,1)最接近
:假定 w ->指定步长->指定拟合次数->进行递归拟合计算
:根据 w 计算 sigmoid 值->计算 sigmoid 值与已知分类的差->计算得出对当前 w 的改进->回到开始
:对 sigmoid 的要求:有定义&可微
:w = w-af(w),f(w)跟当前误差及训练数据有关，f(w) = x * error

岭回归

逻辑回归

分类回归树CARG

聚类：

k-Mean

最大期望

关联：

apriori

FP-GROWTH

序列：

HMM

集成模式：

adaboost